
<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-52138338-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-52138338-2');
    </script>


    <meta name="generator" content="HTML Tidy"></meta>
    <link href="stylesheets/style.css" rel="stylesheet" type="text/css"></link>
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css"></link>
    <script src="js/hidebib.js" type="text/javascript"></script>
    <title>Yogesh Balaji</title>
    <description content="PhD Candidate in Computer Science"></description>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
    <td>
    <table width="100%" align="center" cellspacing="0" cellpadding="20">
      <p align="center">
        <font size="7">Yogesh Balaji</font>
        <br></br>
        <b>Email </b>
        <font id="email" style="display:inline;">ybalaji (at) nvidia (dot) com</font>
      </p>
      <td width="65%" valign="middle" align="justify">
        <div id="includedContent"></div>
        <p>
          <p>
              I am a research scientist at Nvidia Research. I graduated with a PhD in computer science from the
              <a href="https://www.umd.edu/">University of Maryland, College Park</a>, where I was advised by
              <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/"> Prof. Rama Chellappa </a> and
              <a href="https://www.cs.umd.edu/~sfeizi/"> Prof. Soheil Feizi </a>. Prior to joining UMD,
              I obtained a Bachelor's degree (B.Tech) with a major in Electrical Engineering and a minor in
              Operations Research from <a href="https://www.iitm.ac.in/"> Indian Institute of Technology, Madras </a>.
          </p>
          <p>
              I work on problems in machine learning and computer vision, more specifically on multimodal
            learning, large-scale pretraining methods and generative models.
          </p>

          <p>
              My team is looking to hire interns for 2022. If you are interested, please reach out to me.
          </p>
        </p>

        <p align="center"> <a href="https://scholar.google.com/citations?user=0I2qH0oAAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/yogeshbalaji">Github</a> | <a href="https://twitter.com/yogeshbalaji95">Twitter</a> | <a href="https://www.linkedin.com/in/yogesh-balaji-0847b374/">LinkedIn</a></p>
        <!-- <p align="center"> <a href="docs/Yogesh_Balaji_cv.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=0I2qH0oAAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/yogeshbalaji">Github</a> | <a href="https://twitter.com/yogeshbalaji95">Twitter</a> | <a href="https://www.linkedin.com/in/yogesh-balaji-0847b374/">LinkedIn</a></p> -->
      </td>
      <td width="35%" valign="top">
        <a href="images/yogesh_img.jpg">
          <img src="images/yogesh_img.jpg" width="90%" />
        </a>
      </td>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading id="news">News</sectionheading>
          <ul>
              <li>Joined Nvidia as a research scientist.</li>
              <li>One paper accepted at ICLR 2021.</li>
              <li>One paper accepted at AAAI 2021.</li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Research</sectionheading>
        </td>
      </tr>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tr>
        <td>
          <h2> <font color="gray">2021 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openreview.net/pdf?id=C3qvk5IQIJY">
          <a href="https://openreview.net/pdf?id=C3qvk5IQIJY">
            <img src="images/teaser_fig/overparam.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://openreview.net/pdf?id=C3qvk5IQIJY" id="2021ICLRoverparam_paper"><paper_title>Understanding Overparameterization in Generative Adversarial Networks</paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Mohammadmahdi Sajedi, Neha Kalibhat, Mucong Ding, Dominik Stoger, Mahdi Soltanolkotabi, Soheil Feizi.<br> To appear in <em>ICLR</em>, 2021<br><div class="paper" id="2021ICLRoverparam">
            <a href="https://openreview.net/pdf?id=C3qvk5IQIJY">pdf</a>
            | <a href="javascript:toggleblock('2021ICLRoverparam_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2021ICLRoverparam')">bibtex</a>
            | <a href="projectpages/overparam/overparam.html">Project page</a>
            <p align="justify"><i id="2021ICLRoverparam_abs">
              A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs)
              involve training of overparameterized models where the number of parameters of the model exceeds a
              certain threshold. Indeed, most successful GANs used in practice are trained using overparameterized
              generator and discriminator networks, both in terms of depth and width. A large body of work in
              supervised learning have shown the importance of model overparameterization in the convergence of
              the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and
              GANs in particular involve non-convex concave mini-max optimization problems that are often trained
              using Gradient Descent/Ascent (GDA). The role and benefits of model overparameterization in the
              convergence of GDA to a global saddle point in non-convex concave problems is far less understood.
              In this work, we present a comprehensive analysis of the importance of model overparameterization in
              GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model
              with a -layer neural network generator and a linear discriminator, GDA converges to a global saddle
              point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the
              first result for global convergence of GDA in such settings. Our theory is based on a more general
              result that holds for a broader class of nonlinear generators and discriminators that obey certain
              assumptions (including deeper generators and random feature discriminators). Our theory utilizes and
              builds upon a novel connection with the convergence analysis of linear time-varying dynamical systems
              which may have broader implications for understanding the convergence behavior of GDA for non-convex
              concave problems involving overparameterized models. We also empirically study the role of model
              overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets.
              Our experiments show that overparameterization improves the quality of generated samples across
              various model architectures and datasets. Remarkably, we observe that overparameterization leads to
              faster and more stable convergence behavior of GDA across the board.
            </i></p><pre xml:space="preserve">
            @inproceedings{
              balaji2021understanding,
              title={Understanding Over-parameterization
                in Generative Adversarial Networks},
              author={Yogesh Balaji and Mohammadmahdi Sajedi and
                Neha Mukund Kalibhat and Mucong Ding and
                Dominik St{\"o}ger and Mahdi Soltanolkotabi
                and Soheil Feizi},
              booktitle={International Conference on Learning
               Representations},
              year={2021},
              url={https://openreview.net/forum?id=C3qvk5IQIJY}
              }
          </pre></div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2010.02350">
          <a href="https://arxiv.org/abs/2010.02350">
            <img src="images/teaser_fig/lottery_teaser.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/2010.02350" id="2020arxivlottery_paper"><paper_title>Winning Lottery Tickets in Deep Generative Models</paper_title></a>
          <br>Neha Mukund Kalibhat, <strong>Yogesh Balaji</strong>, Soheil Feizi. <br> To appear in <em>AAAI</em>, 2021<br><div class="paper" id="2020arxivlottery">
            <a href="https://arxiv.org/abs/2010.02350">pdf</a>
            | <a href="javascript:toggleblock('2020arxivlottery_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020arxivlottery')">bibtex</a><br>
            <p align="justify"><i id="2020arxivlottery_abs">
              The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized
              properly, can be trained to reach comparable or even better performance to that of the original network.
              Prior works in lottery tickets have primarily focused on the supervised learning setup, with several
              papers proposing effective ways of finding "winning tickets" in classification problems. In this paper,
              we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show
              that the popular iterative magnitude pruning approach (with late rewinding) can be used with generative
              losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99% for
              AutoEncoders, 93% for VAEs and 89% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the
              transferability of winning tickets across different generative models (GANs and VAEs) sharing the same
              architecture, suggesting that winning tickets have inductive biases that could help train a wide range
              of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative
              models by detecting tickets at very early stages in training called "early-bird tickets". Through
              early-bird tickets, we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54%
              reduction in training time, making it possible to train large-scale generative models over tight
              resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan,
              and Torr 2019) and GraSP (Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of
              proper network initializations that could improve convergence and stability of generative models.
            </i></p><pre xml:space="preserve">
            @article{kalibhat2020winning,
            title={Winning Lottery Tickets
             in Deep Generative Models},
            author={Kalibhat, Neha Mukund and
            Balaji, Yogesh and Feizi, Soheil},
            journal={arXiv preprint arXiv:2010.02350},
            year={2020}
          }
          </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2020 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2010.05862">
          <a href="https://arxiv.org/abs/2010.05862">
            <img src="images/teaser_fig/ROT_teaser.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/2010.05862" id="2020NeurIPSRobustOT_paper"><paper_title>Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation</paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Rama Chellappa, Soheil Feizi.<br><em>NeurIPS</em>, 2020<br><div class="paper" id="2020NeurIPSRobustOT">
            <a href="https://arxiv.org/abs/2010.05862">pdf</a>
            | <a href="javascript:toggleblock('2020NeurIPSRobustOT_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020NeurIPSRobustOT')">bibtex</a>
            | <a href="https://github.com/yogeshbalaji/robustOT">code</a><br>
            <p align="justify"><i id="2020NeurIPSRobustOT_abs">
              Optimal Transport (OT) distances such as Wasserstein have been used in several areas such as GANs and
              domain adaptation. OT, however, is very sensitive to outliers (samples with large noise) in the data since
              in its objective function, every sample, including outliers, is weighed similarly due to the marginal
              constraints. To remedy this issue, robust formulations of OT with unbalanced marginal constraints have
              previously been proposed. However, employing these methods in deep learning problems such as GANs and
              domain adaptation is challenging due to the instability of their dual optimization solvers.
              In this paper, we resolve these issues by deriving a computationally-efficient dual form of the robust
              OT optimization that is amenable to modern deep learning applications. We demonstrate the effectiveness
              of our formulation in two applications of GANs and domain adaptation. Our approach can train state-of-the-art
              GAN models on noisy datasets corrupted with outlier distributions. In particular, our optimization computes
              weights for training samples reflecting how difficult it is for those samples to be generated in the model.
              In domain adaptation, our robust OT formulation leads to improved accuracy compared to the standard adversarial
              adaptation methods.
            </i></p><pre xml:space="preserve">
            @misc{balaji2020robust,
             title={Robust Optimal Transport
              with Applications in Generative Modeling
              and Domain Adaptation},
              author={Yogesh Balaji and Rama Chellappa
              and Soheil Feizi},
              year={2020},
              eprint={2010.05862},
              archivePrefix={arXiv},
              primaryClass={cs.LG}
              }
          </pre></div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2010.02418">
          <a href="https://arxiv.org/abs/2010.02418">
            <img src="images/teaser_fig/CAR_teaser.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="https://arxiv.org/abs/2010.02418" id="2020arxivCAR_paper"><paper_title>The Effectiveness of Memory Replay in Large Scale Continual Learning</paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Mehrdad Farajtabar, Dong Yin, Alex Mott, Ang Li.<br><em>arXiv</em>, 2020<br><div class="paper" id="2020arXivCAR">
            <a href="https://arxiv.org/abs/2010.02418">pdf</a>
            | <a href="javascript:toggleblock('2020arXivCAR_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020arXivCAR')">bibtex</a>
            | <a href="images/CAR_talk.mp4">Talk</a><br>
            <p align="justify"><i id="2020arXivCAR_abs">
              We study continual learning in the large scale setting where tasks in the input sequence are not limited
              to classification, and the outputs can be of high dimension. Among multiple state-of-the-art methods,
              we found vanilla experience replay (ER) still very competitive in terms of both performance and
              scalability, despite its simplicity. However, a degraded performance is observed for ER with small
              memory. A further visualization of the feature space reveals that the intermediate representation
              undergoes a distributional drift. While existing methods usually replay only the input-output pairs,
              we hypothesize that their regularization effect is inadequate for complex deep models and diverse tasks
              with small replay buffer size. Following this observation, we propose to replay the activation of the
              intermediate layers in addition to the input-output pairs. Considering that saving raw activation maps
              can dramatically increase memory and compute cost, we propose the Compressed Activation Replay technique,
              where compressed representations of layer activation are saved to the replay buffer. We show that this
              approach can achieve superior regularization effect while adding negligible memory overhead to replay
              method. Experiments on both the large-scale Taskonomy benchmark with a diverse set of tasks and standard
              common datasets (Split-CIFAR and Split-miniImageNet) demonstrate the effectiveness of the proposed method.
            </i></p><pre xml:space="preserve">
            @article{balaji2020effectiveness,
              title={The Effectiveness of Memory Replay
              in Large Scale Continual Learning},
              author={Balaji, Yogesh and Farajtabar,
              Mehrdad and Yin, Dong and Mott, Alex
              and Li, Ang},
              journal={arXiv preprint arXiv:2010.02418},
              year={2020}
            }
          </pre></div></td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540290.pdf">
          <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540290.pdf">
            <img src="images/teaser_fig/DMG.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top">
          <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540290.pdf" id="2020EccvDMG_paper"><paper_title>Learning to Balance Specificity and Invariance for In and Out of Domain Generalization</paper_title></a>
          <br>Prithvijit Chattopadhyay, <strong>Yogesh Balaji</strong>, Judy Hoffman.<br><em>ECCV</em>, 2020<br><div class="paper" id="2020EccvDMG">
            <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540290.pdf">pdf</a>
            | <a href="javascript:toggleblock('2020EccvDMG_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020EccvDMG')">bibtex</a>
            | <a href="https://github.com/prithv1/DMG">code</a> |
            <a href="https://www.youtube.com/watch?v=u4YQdV8dJWM">video</a><br>
            <p align="justify"><i id="2020EccvDMG_abs">
              We introduce Domain-specific Masks for Generalization, a model for improving both in-domain and
              out-of-domain generalization performance. For domain generalization,
              the goal is to learn from a set of source domains to produce a single model that will best
              generalize to an unseen target domain. As such, many prior approaches focus on learning representations
              which persist across all source domains with the assumption that these domain agnostic representations
              will generalize well. However, often individual domains contain characteristics which are unique and when
              leveraged can significantly aid in-domain recognition performance. To produce a model which best
              generalizes to both seen and unseen domains, we propose learning domain specific masks. The masks
              are encouraged to learn a balance of domain-invariant and domain-specific features, thus enabling a
              model which can benefit from the predictive power of specialized features while retaining the universal
              applicability of domain-invariant features. We demonstrate competitive performance compared to naive
              baselines and state-of-the-art methods on both PACS and DomainNet.
            </i></p><pre xml:space="preserve">
            @inproceedings{2020EccvDMG,
            author = {Chattopadhyay, Prithvijit and
                Balaji, Yogesh and Hoffman,
                Judy},
            title = {Learning to Balance Specificity
                and Invariance for In and Out
                of Domain Generalization},
            year = 2020,
            booktitle = {European Conference in Computer
                Vision (ECCV)}
            }
          </pre></div></td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/2007.01261">
          <a href="https://arxiv.org/abs/2007.01261">
            <img src="images/teaser_fig/CMSS.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/2007.01261" id="2020EccvCMSS_paper">
          <paper_title>Curriculum Manager for Source Selection in Multi-Source Domain Adaptation</paper_title></a>
          <br>Luyu Yang, <strong>Yogesh Balaji</strong>, Ser-Nam Lim, Abhinav Shrivastava.<br><em>ECCV</em>, 2020<br>
          <div class="paper" id="2020EccvCMSS"><a href="https://arxiv.org/abs/2007.01261">pdf</a>
            | <a href="javascript:toggleblock('2020EccvCMSS_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020EccvCMSS')">bibtex</a>
              <br><p align="justify">
              <i id="2020EccvCMSS_abs">
                  The performance of Multi-Source Unsupervised Domain Adaptation depends significantly on the effectiveness
                  of transfer from labeled source domain samples. In this paper, we proposed an adversarial agent that
                  learns a dynamic curriculum for source samples, called Curriculum Manager for Source Selection (CMSS).
                  The Curriculum Manager, an independent network module, constantly updates the curriculum during training,
                  and iteratively learns which domains or samples are best suited for aligning to the target. The intuition
                  behind this is to force the Curriculum Manager to constantly re-measure the transferability of latent domains
                  over time to adversarially raise the error rate of the domain discriminator. CMSS does not require any knowledge
                  of the domain labels, yet it outperforms other methods on four well-known benchmarks by significant margins.
                  We also provide interpretable results that shed light on the proposed method.
              </i></p><pre xml:space="preserve">
              @article{2020EccvCMSS,
              title={Curriculum Manager for Source Selection
                  in Multi-Source Domain Adaptation},
              author={Yang, Luyu and Balaji, Yogesh and
                  Lim, Ser-Nam and Shrivastava, Abhinav},
              journal={arXiv preprint arXiv:2007.01261},
              year={2020}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/1910.08051">
          <a href="https://arxiv.org/abs/1910.08051">
            <img src="images/teaser_fig/IAAT.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/1910.08051" id="2020arXivInstanceAdaptive_paper">
          <paper_title>Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets</paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Tom Goldstein, Judy Hoffman.<br><em>arXiv</em>, 2020<br>
          <div class="paper" id="2020arXivInstanceAdaptive"><a href="https://arxiv.org/abs/1910.08051">pdf</a>
            | <a href="javascript:toggleblock('2020arXivInstanceAdaptive_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020arXivInstanceAdaptive')">bibtex</a>
            | <a href="https://github.com/yogeshbalaji/Instance_Adaptive_Adversarial_Training">code</a>
            | <a href="projectpages/adaptive_adversarial/adaptive_adversarial.html">Project page</a> <br><p align="justify">
              <i id="2020arXivInstanceAdaptive_abs">
                Adversarial training is by far the most successful strategy for improving robustness of neural networks
                to adversarial attacks. Despite its success as a defense mechanism, adversarial training fails to
                generalize well to unperturbed test set. We hypothesize that this poor generalization is a consequence
                of adversarial training with uniform perturbation radius around every training sample. Samples close
                to decision boundary can be morphed into a different class under a small perturbation budget, and enforcing
                large margins around these samples produce poor decision boundaries that generalize poorly. Motivated
                by this hypothesis, we propose instance adaptive adversarial training -- a technique that enforces
                sample-specific perturbation margins around every training sample. We show that using our approach,
                test accuracy on unperturbed samples improve with a marginal drop in robustness. Extensive experiments
                on CIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our proposed approach.
              </i></p><pre xml:space="preserve">
              @inproceedings{2020arXivInstanceAdaptive,
              author = {Balaji, Yogesh and Goldstein, Tom
                  and Hoffman, Judy},
              title = {Instance adaptive adversarial
                  training: Improved accuracy
                  tradeoffs in neural nets},
              year = 2020,
              booktitle = {arXiv}
              }
            </pre></div></td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/1911.08654">
          <a href="https://arxiv.org/abs/1911.08654">
            <img src="images/teaser_fig/Adversarial_flow.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/1911.08654" id="2020AISTATSAdvFlow_paper">
          <paper_title>Adversarial Robustness of Flow-Based Generative Models</paper_title></a>
          <br>Phillip Pope*, <strong>Yogesh Balaji*</strong>, Soheil Feizi.<br><em>AISTATS</em>, 2020<br>
          <div class="paper" id="2020AISTATSAdvFlow"><a href="https://arxiv.org/abs/1911.08654">pdf</a>
            | <a href="javascript:toggleblock('2020AISTATSAdvFlow_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2020AISTATSAdvFlow')">bibtex</a><br><p align="justify">
              <i id="2020AISTATSAdvFlow_abs">
                Flow-based generative models leverage invertible generator functions to fit a distribution to the training
                  data using maximum likelihood. Despite their use in several application domains, robustness of these
                  models to adversarial attacks has hardly been explored. In this paper, we study adversarial robustness
                  of flow-based generative models both theoretically (for some simple models) and empirically (for more complex ones).
                  First, we consider a linear flow-based generative model and compute optimal sample-specific and universal
                  adversarial perturbations that maximally decrease the likelihood scores. Using this result, we study the
                  robustness of the well-known adversarial training procedure, where we characterize the fundamental
                  trade-off between model robustness and accuracy. Next, we empirically study the robustness of two
                  prominent deep, non-linear, flow-based generative models, namely GLOW and RealNVP. We design two types
                  of adversarial attacks; one that minimizes the likelihood scores of in-distribution samples, while the
                  other that maximizes the likelihood scores of out-of-distribution ones. We find that GLOW and RealNVP
                  are extremely sensitive to both types of attacks. Finally, using a hybrid adversarial training procedure,
                  we significantly boost the robustness of these generative models.
              </i></p><pre xml:space="preserve">
              @inproceedings{pope2020adversarial,
                  title={Adversarial Robustness of
                  Flow-Based Generative Models},
                  author={Pope, Phillip and Balaji, Yogesh
                  and Feizi, Soheil},
                  booktitle={International Conference on
                  Artificial Intelligence and Statistics},
                  pages={3795--3805},
                  year={2020}
                }
            </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2019 </font> </h2>
        </td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf">
          <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf">
            <img src="images/teaser_fig/NW.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf" id="2019IccvNW_paper">
          <paper_title>Normalized Wasserstein Distance for Mixture Distributions with Applications in Adversarial Learning and Domain Adaptation</paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Rama Chellappa, Soheil Feizi.<br><em>ICCV</em>, 2019<br>
          <div class="paper" id="2019IccvNW"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2019IccvNW_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2019IccvNW')">bibtex</a>
            | <a href="https://github.com/yogeshbalaji/Normalized-Wasserstein">code</a>
            | <a href="projectpages/normalized_wasserstein/NW.html">Project page</a><br><p align="justify">
              <i id="2019IccvNW_abs">
                Understanding proper distance measures between distributions is at the core of several learning tasks
                  such as generative models, domain adaptation, clustering, etc. In this work, we focus on mixture
                  distributions that arise naturally in several application domains where the data contains different
                  sub-populations. For mixture distributions, established distance measures such as the Wasserstein
                  distance do not take into account imbalanced mixture proportions. Thus, even if two mixture
                  distributions have identical mixture components but different mixture proportions, the Wasserstein
                  distance between them will be large. This often leads to undesired results in distance-based learning
                  methods for mixture distributions. In this paper, we resolve this issue by introducing the Normalized
                  Wasserstein measure. The key idea is to introduce mixture proportions as optimization variables,
                  effectively normalizing mixture proportions in the Wasserstein formulation. Using the proposed normalized
                  Wasserstein measure leads to significant performance gains for mixture distributions with imbalanced
                  mixture proportions compared to the vanilla Wasserstein distance. We demonstrate the effectiveness
                  of the proposed measure in GANs, domain adaptation and adversarial clustering in several benchmark
                  datasets.
              </i></p><pre xml:space="preserve">
              @InProceedings{Balaji_2019_ICCV,
                author = {Balaji, Yogesh and Chellappa,
                  Rama and Feizi, Soheil},
                title = {Normalized Wasserstein for
                  Mixture Distributions With
                  Applications in Adversarial
                  Learning and Domain Adaptation},
                booktitle = {Proceedings of the IEEE/CVF
                  International Conference on Computer
                  Vision (ICCV)},
                month = {October},
                year = {2019}
                }
            </pre></div></td>
      </tr>


      <tr>
        <td width="30%" valign="top" align="center" href="https://www.ijcai.org/Proceedings/2019/276">
          <a href="https://www.ijcai.org/Proceedings/2019/276">
            <img src="images/teaser_fig/TFGAN.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://www.ijcai.org/Proceedings/2019/276" id="2019IjcaiTFGAN_paper">
          <paper_title>Conditional GAN with Discriminative Filter Generation for Text-To-Video Synthesis</paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Martin Renqiang Min, Bing Bai, Rama Chellappa, Hans Peter Graf.<br><em>IJCAI</em>, 2019<br>
          <div class="paper" id="2019IjcaiTFGAN"><a href="https://www.ijcai.org/Proceedings/2019/276">pdf</a>
            | <a href="javascript:toggleblock('2019IjcaiTFGAN_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2019IjcaiTFGAN')">bibtex</a><br><p align="justify">
              <i id="2019IjcaiTFGAN_abs">
                Developing conditional generative models for text-to-video synthesis is an extremely challenging yet an
                important topic of research in machine learning. In this work, we address this problem by introducing
                Text-Filter conditioning Generative Adversarial Network (TFGAN), a conditional GAN model with a novel
                multi-scale text-conditioning scheme that improves text-video associations. By combining the proposed
                conditioning scheme with a deep GAN architecture, TFGAN generates high quality videos from text on
                challenging real-world video datasets. In addition, we construct a synthetic dataset of text-conditioned
                moving shapes to systematically evaluate our conditioning scheme. Extensive experiments demonstrate that
                TFGAN significantly outperforms existing approaches, and can also generate videos of novel categories not
                seen during training.
              </i></p><pre xml:space="preserve">
              @inproceedings{ijcai2019-276,
                title     = {Conditional GAN with Discriminative
              Filter Generation for Text-to-Video Synthesis},
                author    = {Balaji, Yogesh and Min, Martin Renqiang
              and Bai, Bing and Chellappa, Rama and
              Graf, Hans Peter},
                booktitle = {Proceedings of the Twenty-Eighth
              International Joint Conference on
              Artificial Intelligence, {IJCAI-19}},
                year      = {2019}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://arxiv.org/abs/1810.04147">
          <a href="https://arxiv.org/abs/1810.04147">
            <img src="images/teaser_fig/EGAN.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://arxiv.org/abs/1810.04147" id="2019IcmlEGAN_paper">
          <paper_title>Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs </paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Hamed Hassani, Rama Chellappa, Soheil Feizi.<br><em>ICML</em>, 2019<br>
          <div class="paper" id="2019IcmlEGAN"><a href="https://arxiv.org/abs/1810.04147">pdf</a>
            | <a href="javascript:toggleblock('2019IcmlEGAN_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2019IcmlEGAN')">bibtex</a>
            | <a href="https://github.com/yogeshbalaji/EntropicGANs_meet_VAEs">code</a>
            | <a href="https://icml.cc/media/Slides/icml/2019/halla(11-14-00)-11-15-00-4626-entropic_gans_m.pdf">Slides</a>
            | <a href="https://youtube.videoken.com/embed/HlyE7P7gxYE?tocitem=64">Talk</a><br><p align="justify">
              <i id="2019IcmlEGAN_abs">
                Building on the success of deep learning, two
                modern approaches to learn a probability model
                from the data are Generative Adversarial Networks (GANs) and Variational AutoEncoders
                (VAEs). VAEs consider an explicit probability
                model for the data and compute a generative distribution by maximizing a variational lower-bound
                on the log-likelihood function. GANs, however,
                compute a generative model by minimizing a distance between observed and generated probability
                distributions without considering an explicit
                model for the observed data. The lack of having explicit probability models in GANs prohibits
                computation of sample likelihoods in their frameworks and limits their use in statistical inference
                problems. In this work, we resolve this issue by
                constructing an explicit probability model that can
                be used to compute sample likelihood statistics
                in GANs. In particular, we prove that under this
                probability model, a family of Wasserstein GANs
                with an entropy regularization can be viewed as
                a generative model that maximizes a variational
                lower-bound on average sample log likelihoods,
                an approach that VAEs are based on. This result
                makes a principled connection between two modern generative models, namely GANs and VAEs.
                In addition to the aforementioned theoretical results, we compute likelihood statistics for GANs
                trained on Gaussian, MNIST, SVHN, CIFAR-10
                and LSUN datasets. Our numerical results validate the proposed theory.

              </i></p><pre xml:space="preserve">
              @InProceedings{pmlr-v97-balaji19a,
                title = 	 {Entropic {GAN}s meet {VAE}s:
              A Statistical Approach to Compute Sample
              Likelihoods in {GAN}s},
                author =    {Balaji, Yogesh and Hassani,
              Hamed and Chellappa, Rama and Feizi, Soheil},
                pages = 	 {414--423},
                year = 	 {2019},
                series = 	 {Proceedings of Machine
              Learning Research},
                address = 	 {Long Beach, California, USA},
                month = 	 {09--15 Jun},
                publisher =    {PMLR}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2018 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization">
          <a href="https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization">
            <img src="images/teaser_fig/Metareg.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization" id="2018NeuripsMetareg_paper">
          <paper_title>MetaReg: Towards Domain Generalization using Meta-Regularization </paper_title></a>
          <br><strong>Yogesh Balaji</strong>, Swami Sankaranarayanan, Rama Chellappa.<br><em>NeurIPS</em>, 2018<br>
          <div class="paper" id="2018NeuripsMetareg"><a href="https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization">pdf</a>
            | <a href="javascript:toggleblock('2018NeuripsMetareg_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2018NeuripsMetareg')">bibtex</a><br><p align="justify">
              <i id="2018NeuripsMetareg_abs">
                Training models that generalize to new domains at test time is a problem of fundamental importance
                in machine learning. In this work, we encode this notion of domain generalization using a novel
                regularization function. We pose the problem of finding such a regularization function in a Learning
                to Learn (or) meta-learning framework. The objective of domain generalization is explicitly modeled
                by learning a regularizer that makes the model trained on one domain to perform well on another domain.
                Experimental validations on computer vision and natural language datasets indicate that our method can
                learn regularizers that achieve good cross-domain generalization.

              </i></p><pre xml:space="preserve">
              @incollection{NIPS2018_7378,
                title = {MetaReg: Towards Domain
              Generalization using Meta-Regularization},
                author = {Balaji, Yogesh and Sankaranarayanan,
              Swami and Chellappa, Rama},
                booktitle = {Advances in Neural Information
              Processing Systems 31},
                pages = {998--1008},
                year = {2018},
                publisher = {Curran Associates, Inc.},
                }
            </pre></div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.pdf">
          <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.pdf">
            <img src="images/teaser_fig/GTA.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.pdf" id="2018CvprGTA_paper">
          <paper_title>Generate To Adapt: Aligning Domains using Generative Adversarial Networks </paper_title> </a> <font color="red"> (Spotlight Oral)</font>
          <br>Swami Sankaranarayan*, <strong>Yogesh Balaji*</strong>, Carlos D. Castillo, Rama Chellappa.<br><em>CVPR</em>, 2018<br>
          <div class="paper" id="2018CvprGTA"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2018CvprGTA_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2018CvprGTA')">bibtex</a>
            | <a href="https://github.com/yogeshbalaji/Generate_To_Adapt">code</a>
            | <a href="https://youtu.be/HAOCaqvcf8w?t=2998">Talk</a><br><p align="justify">
              <i id="2018CvprGTA_abs">
                Domain Adaptation is an actively researched problem in
                Computer Vision. In this work, we propose an approach
                that leverages unsupervised data to bring the source and
                target distributions closer in a learned joint feature space.
                We accomplish this by inducing a symbiotic relationship between
                the learned embedding and a generative adversarial
                network. This is in contrast to methods which use the adversarial framework
                for realistic data generation and retraining deep models with such data. We demonstrate the
                strength and generality of our approach by performing experiments on three different
                tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS
                datasets) (2) Object recognition using OFFICE dataset and
                (3) Domain adaptation from synthetic to real data. Our
                method achieves state-of-the art performance in most experimental settings and by far the
                only GAN-based method that has been shown to work well across different datasets
                such as OFFICE and DIGITS

              </i></p><pre xml:space="preserve">
              @inproceedings{sankaranarayanan2018generate,
                title={Generate to adapt: Aligning domains
              using generative adversarial networks},
                author={Sankaranarayanan, Swami and Balaji, Yogesh
              and Castillo, Carlos D and Chellappa, Rama},
                booktitle={Proceedings of the IEEE Conference
              on Computer Vision and Pattern Recognition},
                pages={8503--8512},
                year={2018}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf">
          <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf">
            <img src="images/teaser_fig/LSDseg.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf" id="2018CvprLSD_paper">
          <paper_title> Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation </paper_title></a> <font color="red"> (Spotlight Oral)</font>
          <br>Swami Sankaranarayan*, <strong>Yogesh Balaji*</strong>, Arpit Jain, Ser Nam Lim, Rama Chellappa.<br><em>CVPR</em>, 2018<br>
          <div class="paper" id="2018CvprLSD"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2018CvprLSD_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2018CvprLSD')">bibtex</a>
            | <a href="https://github.com/swamiviv/LSD-seg">code</a>
            | <a href="https://youtu.be/lNqXyJliVSo?t=3645">Talk</a><br><p align="justify">
              <i id="2018CvprLSD_abs">
                Visual Domain Adaptation is a problem of immense importance in computer vision.
                Previous approaches showcase the inability of even deep neural networks to learn informative
                representations across domain shift. This problem is more severe for tasks where acquiring hand labeled
                data is extremely hard and tedious. In this work, we focus
                on adapting the representations learned by segmentation
                networks across synthetic and real domains. Contrary to
                previous approaches that use a simple adversarial objective
                or superpixel information to aid the process, we propose
                an approach based on Generative Adversarial Networks
                (GANs) that brings the embeddings closer in the learned
                feature space. To showcase the generality and scalability of
                our approach, we show that we can achieve state of the art
                results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory
                experiments show that our approach: (1) generalizes to unseen domains and
                (2) results in improved alignment of source and target distributions.

              </i></p><pre xml:space="preserve">
              @InProceedings{Sankaranarayanan_2018_CVPR,
              author = {Sankaranarayanan, Swami and Balaji, Yogesh
              and Jain, Arpit and Lim, Ser Nam and Chellappa, Rama},
              title = {Learning From Synthetic Data: Addressing Domain
              Shift for Semantic Segmentation},
              booktitle = {Proceedings of the IEEE Conference on Computer
              Vision and Pattern Recognition (CVPR)},
              month = {June},
              year = {2018}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2017 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rengarajan_Unrolling_the_Shutter_CVPR_2017_paper.pdf">
          <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rengarajan_Unrolling_the_Shutter_CVPR_2017_paper.pdf">
            <img src="images/teaser_fig/RS.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rengarajan_Unrolling_the_Shutter_CVPR_2017_paper.pdf" id="2017CvprRS_paper">
          <paper_title> Unrolling the Shutter: CNN to Correct Motion Distortions </paper_title></a> <font color="red"> (Oral)</font>
          <br>Vijay Rengarajan, <strong>Yogesh Balaji*</strong>, A. N. Rajagopalan.<br><em>CVPR</em>, 2017<br>
          <div class="paper" id="2017CvprRS"><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Rengarajan_Unrolling_the_Shutter_CVPR_2017_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2017CvprRS_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2017CvprRS')">bibtex</a>
            | <a href="https://github.com/yogeshbalaji/CVPR17_Unrolling_the_shutter">code</a>
            | <a href="https://apvijay.github.io/rs_rect_cnn.html">Project page</a>
            | <a href="https://www.youtube.com/watch?v=Hcz-h_yut84&ab_channel=ComputerVisionFoundationVideos">Talk</a><br><p align="justify">
              <i id="2017CvprRS_abs">
                Row-wise exposure delay present in CMOS cameras is responsible for skew and curvature distortions
                known as the rolling shutter (RS) effect while imaging under camera motion. Existing RS correction
                methods resort to using multiple images or tailor scene-specific correction schemes. We propose a
                convolutional neural network (CNN) architecture that automatically learns essential scene features
                from a single RS image to estimate the row-wise camera motion and undo RS distortions back to the
                time of first-row exposure. We employ long rectangular kernels to specifically learn the effects
                produced by the row-wise exposure. Experiments reveal that our proposed architecture performs better
                than the conventional CNN employing square kernels. Our single-image correction method fares well
                even operating in a frame-by-frame manner against video-based methods and performs better than
                scene-specific correction schemes even under challenging situations.

              </i></p><pre xml:space="preserve">
              @inproceedings{UnrollingShutter_CVPR17,
                  Author = {Vijay Rengarajan and Yogesh Balaji
              and A.N. Rajagopalan},
                  Title = {Unrolling the Shutter: CNN to Correct
              Motion Distortions},
                  Booktitle = {Computer Vision and Pattern
              Recognition (CVPR)},
                  Year = {2017}
              }
            </pre></div></td>
      </tr>

      <tr>
        <td>
          <h2> <font color="gray">2016 </font> </h2>
        </td>
      </tr>

      <tr>
        <td width="30%" valign="top" align="center" href="https://abhijithpunnappurath.github.io/eccv_paper.pdf">
          <a href="https://abhijithpunnappurath.github.io/eccv_paper.pdf">
            <img src="images/teaser_fig/D3M.png" alt="sym" width="100%" style="border-style: none" />
          </a>
        </td>
        <td width="70%" valign="top"><a href="https://abhijithpunnappurath.github.io/eccv_paper.pdf" id="2016EccvD3M_paper">
          <paper_title> Deep Decoupling of Defocus and Motion Blur for Dynamic Segmentation </paper_title></a>
          <br>Abhijith Punnappurath, <strong>Yogesh Balaji*</strong>, Mahesh Mohan, A. N. Rajagopalan.<br><em>ECCV</em>, 2016<br>
          <div class="paper" id="2016EccvD3M"><a href="https://abhijithpunnappurath.github.io/eccv_paper.pdf">pdf</a>
            | <a href="javascript:toggleblock('2016EccvD3M_abs')">abstract</a>
            | <a shape="rect" class="togglebib" href="javascript:togglebib('2016EccvD3M')">bibtex</a>
            | <a href="https://abhijithpunnappurath.github.io/d3m.html">Project page</a> <br><p align="justify">
              <i id="2016EccvD3M_abs">
                We address the challenging problem of segmenting dynamic
                objects given a single space-variantly blurred image of a 3D scene captured using a hand-held camera.
                The blur induced at a particular pixel
                on a moving object is due to the combined effects of camera motion, the
                objects own independent motion during exposure, its relative depth in
                the scene, and defocusing due to lens settings. We develop a deep convolutional
                neural network (CNN) to predict the probabilistic distribution
                of the composite kernel which is the convolution of motion blur and
                defocus kernels at each pixel. Based on the defocus component, we segment the image into
                different depth layers. We then judiciously exploit
                the motion component present in the composite kernels to automatically
                segment dynamic objects at each depth layer. Jointly handling defocus
                and motion blur enables us to resolve depth-motion ambiguity which has
                been a major limitation of the existing segmentation algorithms. Experimental evaluations on
                synthetic and real data reveal that our method
                significantly outperforms contemporary techniques.

              </i></p><pre xml:space="preserve">
              @inproceedings{punnappurath2016deep,
                title={Deep decoupling of defocus
              and motion blur for dynamic
              segmentation},
                author={Punnappurath, Abhijith
              and Balaji, Yogesh
              and Mohan, Mahesh and
              Rajagopalan, Ambasamudram Narayanan},
                booktitle={European Conference on
              Computer Vision},
                pages={750--765},
                year={2016},
                organization={Springer}
              }
            </pre></div></td>
      </tr>
      

    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Internships</sectionheading>
          <ul>
            <li> <a href="https://deepmind.com/">Deepmind </a> (Summer 2020) <br>Topic: Large Scale Continual Learning </li>
            <li> <a href="https://ai.facebook.com/">Facebook AI Research</a> (Summer 2019) <br>Topic: Adversarial Robustness </li>
            <li> <a href="http://www.nec-labs.com/">NEC Laboratories America</a> (Summer 2018) <br>Topic: Text-conditioned Video GANs </li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td>
          <sectionheading>Selected Awards</sectionheading>
          <ul>
            <li> Ann G. Wylie Dissertation Fellowship (2020)</li>
            <li> Dean's Fellowship (2016-2018)</li>
          </ul>
        </td>
      </tr>
    </table>
  </body>
  <script xml:space="preserve" language="JavaScript">hideallbibs();</script>
  <script xml:space="preserve" language="javascript">hideblock('2020arXivCAR_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2021ICLRoverparam_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020arxivlottery_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020NeurIPSRobustOT_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020EccvDMG_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020EccvCMSS_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020AISTATSAdvFlow_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2020arXivInstanceAdaptive_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2019IccvNW_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2019IjcaiTFGAN_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2019IcmlEGAN_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2018NeuripsMetareg_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2018CvprGTA_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2018CvprLSD_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2017CvprRS_abs');</script>
  <script xml:space="preserve" language="javascript">hideblock('2016EccvD3M_abs');</script>

</html>
